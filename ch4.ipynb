{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a95efab-ca5d-400e-91c1-5bc240baf6df",
   "metadata": {},
   "source": [
    "# 딥러닝 기초\n",
    "\n",
    "## CH4 과제\n",
    "\n",
    "## 2020100381 안정빈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a506351-bb2f-409d-8dfa-c81206c4d170",
   "metadata": {},
   "source": [
    "## Ch4. 신경망 학습\n",
    "+ 학습: 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것\n",
    "+ 학습의 목표: 손실 함수의 결괏값을 가장 작게 만드는 가중치 매개변수를 찾는 것\n",
    "+ 신경망은 가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다.\n",
    "+ 신경망은 수천에서 수만개의 매개변수가 존재하기에, 매개 변수를 수작업이 아닌 자동으로 설정할 수 있는 것은 매우 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08275d4d-97fd-4438-b1a6-d60964903cad",
   "metadata": {},
   "source": [
    "### 4-1-1. 데이터 주도 학습\n",
    "+ 머신러닝은 데이터에서 답을 찾고 데이터에서 패턴을 발견하고 데이터로 이야기를 만드는 것으로, 데이터가 매우 중요한 요소이다.\n",
    "+ 사람은 경험과 직관을 단서로, 시행착오를 거듭하며 일을 진행한다.\n",
    "+ 반면, 머신러닝에서는 사람의 개입을 최소화하고 수집한 데이터로부터 패턴을 찾으려 시도한다.\n",
    "+ 신경망과 딥러닝은 기존 머신러닝에서 사용하던 방법보다 사람의 개입을 더욱 배제할 수 있게 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78c3a0d-931a-46ef-ad85-1a9305f06237",
   "metadata": {},
   "source": [
    "손 글씨 숫자를 분류하는 경우:\n",
    "+ 5-> 사람이 생각한 알고리즘 -> 결과\n",
    "+ 5-> 사람이 생각한 특징 추출(SIFT, HOG 등) -> 머신러닝 알고리즘(SVM, KNN) -> 결과\n",
    "+ 5->신경망(딥러닝) -> 결과\n",
    "\n",
    "즉, 머신러닝은 특징을 사람이 설계하지만, 신경망은 이미지를 있는 그대로 학습하여, 이미지에 포함된 중요한 특징까지도 '기계'가 스스로 학습한다.\n",
    "\n",
    "이러한 특성으로 딥러닝을 종단간 머신러닝이라고도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62205736-c55f-481e-947e-45517b5e5a7b",
   "metadata": {},
   "source": [
    "### 4-1-2. 훈련 데이터와 시험 데이터\n",
    "+ 머신러닝은 데이터를 훈련 데이터와 시험 데이터로 나눠, 훈련 데이터로 최적의 매개변수를 찾고, 시험 데이터로 모델의 실력을 평가한다.\n",
    "+ 범용 능력(아직 보지 못한 데이터로도 문제를 풀어내는 능력)을 평가하기 위해 데이터를 분할\n",
    "+ 훈련 데이터에만 지나치게 최적화된 상태(과대 적합)여부를 확인, 과대적합을 피하는 것은 머신러닝의 중요한 과제이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d57676-6d91-48a7-b8e5-1f88612eb69b",
   "metadata": {},
   "source": [
    "### 4-2. 손실함수\n",
    "+ 신경망 학습에서는 현재의 상태를 '하나의 지표'로 표현한다. 그리고 그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색한다.\n",
    "+ 이 때 신경망 학습에서 사용하는 지표를 손실 함수라고 한다. 일반적으로 SSE(오차제곱합)과 교차 엔트로피 오차를 사용한다.\n",
    "+ 손실 함수는 신경망의 성능의 '나쁨'을 나타내는 지표로, 현재의 신경망이 훈련 데이터를 얼마나 잘 처리하지 '못' 하느냐를 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddc6fde-0069-4c8b-b16e-386a01146ef1",
   "metadata": {},
   "source": [
    "### 4-2-1. 오차제곱합\n",
    "오차 제곱합: (신경망의 출력-정답 레이블)의 제곱을 모두 더하고 2로 나눈 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ae31fcf8-30c2-4e11-9cd8-a12e62aaf578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09750000000000003\n",
      "0.5975\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "y=[0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0] # 배열의 원소는 첫 번째 인덱스부터 순서대로 숫자 '0','1','2'...일 때의 값\n",
    "# y는 소프트맥스 함수의 출력 \n",
    "t=[0,0,1,0,0,0,0,0,0,0] # 정답을 가리키는 위치의 원소는 1, 그 외에는 0으로 표시\n",
    "# t처럼 한 원소만 1, 나머지 원소는 0으로 나타내는 표기법을 원-핫 인코딩이라 함.\n",
    "\n",
    "def sum_squares_error(y,t): # SSE\n",
    "    return 0.5 * np.sum((y-t)**2)\n",
    "\n",
    "print(sum_squares_error(np.array(y),np.array(t)))\n",
    "\n",
    "y=[0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] # 7d일 확률이 가장 높다고 추정함(0.6)\n",
    "print(sum_squares_error(np.array(y),np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e25fb02-0dda-46fc-a715-e25e6a88e28f",
   "metadata": {},
   "source": [
    "첫 번째의 예는 정답이 2고 신경망의 출력도 2에서 가장 높은 경우, 두번째 예는 정답이 2지만, 신경망의 출력은 7에서 가장 높다.\n",
    "\n",
    "그렇기에 첫 번째 예시의 손실 함수 출력값이 작으며, 정답 레이블과의 오차도 작다는 것을 알 수 있다.\n",
    "\n",
    "-> 오차제곱합 기준으로 첫 번째 추정 결과가 오차가 더 작으므로 정답에 더 가까울 것으로 판단 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb15ea0-e4cb-4ca8-bb4e-fe94bf2914fc",
   "metadata": {},
   "source": [
    "### 4-2-2. 교차 엔트로피 오차\n",
    "정답일 때의 추정의 자연로그를 계산하는 식 \n",
    "ex) 정답 레이블이 2이고, 신경망의 출력이 0.6이라면 교차 엔트로피 오차는 -log0.6=0.51이 된다. / 같은 조건에서 신경망 출력이 0.1이라면 -log0.1=2.30\n",
    "\n",
    "-> 교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cff89e15-5859-4b94-966b-dfbe10b9adf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.510825457099338\n",
      "2.302584092994546\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta=1e-7\n",
    "    return -np.sum(t*np.log(y+delta))\n",
    "\n",
    "t=[0,0,1,0,0,0,0,0,0,0]\n",
    "y=[0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "print(cross_entropy_error(np.array(y),np.array(t)))\n",
    "\n",
    "y=[0.1,0.05,0.1,0.0,0.05,0.1,0.0,0.6,0.0,0.0]\n",
    "print(cross_entropy_error(np.array(y),np.array(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21df383a-081a-4cf2-b78f-812682097109",
   "metadata": {},
   "source": [
    "정답일 때의 출력이 높을 때, 엔트로피 오차가 적게 나온다. -> 결과가 더 작은 첫 번째 추정이 정답일 가능성이 높다고 판단할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19485650-0270-4a04-870f-204f61ab6857",
   "metadata": {},
   "source": [
    "### 4-2-3. 미니배치 학습\n",
    "+ 머신러닝 문제는 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개변수를 찾는다.\n",
    "+ 모든 훈련 데이터를 대상으로 손실 함수 값을 구해야 함\n",
    "+ 훈련 데이터가 100개라 가정, 그로부터 계산한 100개의 손실 함수 값들의 합을 지표로 삼는다.\n",
    "+ 그러나 데이터가 많을 경우, 모든 데이터를 대상으로 손실 함수를 계산하는 것은 현실적이지 않다.\n",
    "-> 데이터 일부를 추려 전체의 '근사치'로 이용. 일부 데이터(미니 배치)를 이용해 학습한다. 이러한 학습 방법을 미니배치 학습이라 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cb5c6d6-4851-474e-8252-985296f44ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), '..'))\n",
    "from dataset.mnist import load_mnist \n",
    "\n",
    "(x_train, t_train),(x_test,t_test)= \\\n",
    "    load_mnist(normalize=True, one_hot_label=True) \n",
    "# normalize: 입력 이미지의 픽셀값을 0~1.0 사이로 정규화 시킬 것인지 여부/ one_hot_label: 정답 위치의 원소만 1이고 나머지가 0으로 처리할 것인지 여부\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d30c4474-2e67-471a-b5e0-36c28fa1a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=x_train.shape[0]\n",
    "batch_size=10\n",
    "batch_mask=np.random.choice(train_size, batch_size) # 지정한 범위의 수 중 무작위로 원하는 개수만 꺼내는 함수\n",
    "# ex) np.random.choice(6000,10) 0이상 6000미만의 수 중에서 무작위로 10개 추출\n",
    "x_batch=x_train[batch_mask]\n",
    "t_batch=t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58130fcc-dd75-4f93-a3a8-a1901f99864a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([26816, 21942, 39576,  6827, 13897, 10160, 52587, 22908, 22769,\n",
       "        1397], dtype=int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6f25e-7080-44b8-b282-baee05efcadc",
   "metadata": {},
   "source": [
    "### 4-2-4. (배치용) 교차 엔트로피 오차 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7aa13b13-a43e-4802-96b5-9cfff92146b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim==1:\n",
    "        t=t.reshape(1, t.size) # t: 정답 레이블\n",
    "        y=y.reshape(1, y.size) # y: 신경망의 출력 \n",
    "\n",
    "    batch_size=y.shape[0]\n",
    "    return -np.sum(t*np.log(y+1e-7)) / batch_size\n",
    "# reshape 함수로 데이터의 형상을 바꿔주고, 배치의 크기로 나눠 정규호하고 이미지 1장당 평균의 교차 엔트로피 오차를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "44b05ef3-4dc6-4c45-a0f8-33eef31c433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 레이블이 원-핫 인코딩이 아닌 숫자 레이블로 주어졌을 때의 교차 엔트로피 오차\n",
    "\n",
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim==1:\n",
    "        t=t.reshape(1, t.size) \n",
    "        y=y.reshape(1, y.size)\n",
    "\n",
    "    batch_size=y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size),t] + 1e-7)) / batch_size\n",
    "# 정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2517f4df-8b35-44cf-8572-21b3d577dcf4",
   "metadata": {},
   "source": [
    "### 4-2-5. 왜 손실 함수를 설정하는가?\n",
    "+ 신경망 학습에서는 최적의 매개변수를 탐색할 때 손실 함수의 값을 가능한 적게 하는 매개변수의 값을 찾는데,\n",
    "+ 이때 매개변수의 미분(기울기)를 계산하고 그 미분 값을 단서로 매개변수의 값을 서서히 갱신하는 과정을 반복한다.\n",
    "+ 만일, 미분 값이 음수면 그 가중치 매개변수를 양의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다\n",
    "+ 반대로 미분 값이 양수면 그 가중치 매개변수를 음의 방향으로 변화시켜 손실 함수의 값을 줄일 수 있다.\n",
    "+ 미분 값이 0이면 가중치 매개변수를 움직여도 손실 함수의 값이 줄어들지 않는데, 정확도를 지표로 하면 매개변수의 미분이 0이 되는 문제가 생긴다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b38df8-4289-4801-b69a-040674c00b13",
   "metadata": {},
   "source": [
    "### 4-3-1. 미분\n",
    "+ 경사법에서는 기울기(경사) 값을 기준으로 나아갈 방향을 정한다.\n",
    "+ 미분이란 '특정 순간'의 변화량을 뜻한다. ex)직전 1분에 달린 거리, 직전 1초에 달린 거리 등 간격을 줄여 한순간(어느 순간의 속도) 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e28da17-8a7e-4507-90a2-894e84b7ddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나쁜 구현 예시 \n",
    "def numerical_diff(f,x): # 수치 미분\n",
    "    h=1e-50\n",
    "    return (f(x+h)-f(x))/ h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "51f5546a-e31d-4053-9804-196d8bbe64dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(np.float32(1e-50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2296de90-3062-4d3b-b58a-f3a51772b0ae",
   "metadata": {},
   "source": [
    "x의 '작은 변화'가 함수 f(x)를 얼마나 변화시키느냐를 의미하는 코드\n",
    "\n",
    "이때 시간을 뜻하는 h를 한없이 0에 가깝게 만든다.\n",
    "\n",
    "잘못된 점:\n",
    "+ h에 가급적 작은 값을 대입하기 위해 설정한 1e-50이라는 작은 값은 반올림 오차 발생\n",
    "\n",
    "  반올림 오차는 작은 값(소수점 8자리 이하)가 생략되어 최종 계산 결과에 오차가 생기게 함.\n",
    "\n",
    "  -> h값을 10의 -4승으로 변경 \n",
    "+ 진정한 미분은 x 위치의 함수의 기울에 해당하나, 위의 구현에서의 미분은 (x+h)와 x 사이의 기울기에 해당 \n",
    "\n",
    "  -> 이 오차를 줄이기 위해 (x+h)와 (x-h)일 때의 함수 f의 차분을 계산하는 방법 사용\n",
    "\n",
    "  이 차분은 x를 중심으로 그 전후의 차분을 계산한다는 의미에서 중심 차분, 중앙 차분이라 부름."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe6729d9-b333-4984-8200-e8d69856e9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 개선\n",
    "def numerical_diff(f,x):\n",
    "    h=1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d57f19-45a5-4604-94cb-338c3150798e",
   "metadata": {},
   "source": [
    "### 4-3-2.수치 미분의 예\n",
    "\n",
    "아주 작은 차분으로 미분하는 것을 의미\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8867de4a-429f-4fd1-b82b-1a2531947bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# y=0.01x²+0.1x 식 미분\n",
    "\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "x=np.arange(0.0,20.0,0.1) # 0에서 20까지 0.1 간격의 배열 x 생성 , 20은 미포함\n",
    "y=function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ebf1b1fb-d7fb-4db1-9277-57225c9bcdd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1999999999990898\n",
      "0.2999999999986347\n"
     ]
    }
   ],
   "source": [
    "# x=5일때 미분 값\n",
    "print(numerical_diff(function_1, 5))\n",
    "# x=10일때 미분 값\n",
    "print(numerical_diff(function_1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259894aa-04a4-41f0-87f1-a9b6311f4d17",
   "metadata": {},
   "source": [
    "위에서 계산한 미분 값이 x에 대한 f(x)의 변화량, 즉 함수의 기울기이다.\n",
    "\n",
    "x가 5와 10일 때의 '진정한 미분'은 0.2와 0.3으로, 앞의 수치 미분과 결과를 비교하면, 그 오차가 매우 작은 것을 확인 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae71129-ee9b-4231-b0bd-cd4b010196d7",
   "metadata": {},
   "source": [
    "### 4-3-3. 편미분\n",
    "\n",
    "변수가 여럿인 함수에 대한 미분\n",
    "\n",
    "+ 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구한다.\n",
    "+ 단, 여러 변수 중 목표 변수 하나에 초점을 맞추고 다른 변수는 값을 고정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6864a33b-f85d-4928-ba11-a5512c55c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인수 x는 넘파이 배열이라 가정. 넘파이 배열의 각 원소를 제곱하고 그 합을 구하는 예시\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47d5bcee-316e-45c5-aa53-3c26b9731e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.00000000000378\n"
     ]
    }
   ],
   "source": [
    "# x0=3, x1=4일때, x0에 대한 편미분\n",
    "\n",
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0 ** 2.0\n",
    "\n",
    "print(numerical_diff(function_tmp1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e87d9c60-e816-491c-9b65-6623703b20b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.999999999999119\n"
     ]
    }
   ],
   "source": [
    "# x0=3, x1=4일때, x1에 대한 편미분\n",
    "\n",
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1*x1\n",
    "\n",
    "print(numerical_diff(function_tmp2,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e1ee4-b950-4ed0-bca5-1750c7736dd7",
   "metadata": {},
   "source": [
    "## 4-4. 기울기\n",
    "\n",
    "모든 변수의 편미분을 벡터로 정리한 것을 의미"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f5cb694-00d0-4b6b-87af-b01efc892bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기 구현\n",
    "def numerical_gradient(f,x):\n",
    "    h=1e-4\n",
    "    grad=np.zeros_like(x) # x와 형상이 같고, 그 원소가 모두 0인 배열 생성\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val=x[idx]\n",
    "\n",
    "        # f(x+h) 계산\n",
    "        x[idx]=tmp_val + h\n",
    "        fxh1= f(x)\n",
    "\n",
    "        # f(x-h) 계산\n",
    "        x[idx]=tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e744e516-ebc8-474f-b81e-100d710ad777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "# (3,4)에서의 기울기\n",
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "\n",
    "# (0,2)에서의 기울기\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "\n",
    "# (3,0)에서의 기울기\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aabf99a-a24e-49a2-b177-017164ac7d20",
   "metadata": {},
   "source": [
    "+ 각 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향을 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101a2682-4bff-4a19-8f61-529bf57b7d4f",
   "metadata": {},
   "source": [
    "### 4-4-1. 경사법\n",
    "\n",
    "+ 머신러닝 문제에서 최적의 매개변수를 찾기 위해선, 손실 함수가 최솟값이 되어야 함.\n",
    "+ 그러나 매개변수 공간이 광대하여 어디가 최솟값이 되는 곳인지 알기 어려움.\n",
    "+ -> 이때, 기울기를 잘 이용해 함수의 최솟값(가능한 작은 값)을 찾는 기법이 경사법\n",
    "+ 각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표는 기울기이다.\n",
    "+ 그러나 기울기가 가리키는 곳에 정말 함수의 최솟값이 있는지, 보장하진 못한다.\n",
    "+ 정리하자면, 기울어진 방향이 최솟값을 보장하진 않으나, 그 방향으로 가야 함수의 값을 줄일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94935869-1818-4ce4-8d55-3b0daae3a531",
   "metadata": {},
   "source": [
    "+ 경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동, 이동한 곳에서도 기울기를 구하고, 기울어진 방향으로 나아감을 반복\n",
    "+ 이러한 방법으로 함수의 값을 점차 줄이는 기법을 경사법이라 한다.\n",
    "+ 경사의 최솟값을 찾으면 경사하강법 / 경사의 최댓값을 찾으면 경사 상승법\n",
    "+ 경사법에선, 매개변수 값을 얼마나 갱신하느냐를 정하는 학습률을 이용한다. 학습률은 사전에 특정 값으로 정의해야 한다.\n",
    "+ 학습률이 너무 크거나, 작으면 좋은 결과를 얻을 수 없다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6db56377-dc03-497d-94d5-264c75e3f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100): # f:최적화 하려는 함수, init_x:초깃값, lr:학습률, step_num:반복 횟수\n",
    "    x=init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad=numerical_gradient(f,x)\n",
    "        x-=lr * grad\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a1ed483e-1c1f-4153-a951-339d2ec53dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.11110793e-10  8.14814391e-10]\n"
     ]
    }
   ],
   "source": [
    "# 초기값을 (-3.0, -4.0)으로 설정한 후 최솟값 탐색\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x=np.array([-3.0,4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7c274f01-4259-45da-91fc-d1172f1fc7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n",
      "[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 너무 큰 예: lr=10.0\n",
    "init_x=np.array([-3.0,4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100))\n",
    "\n",
    "# 학습률이 너무 작은 예: lr=1e-10\n",
    "init_x=np.array([-3.0,4.0])\n",
    "print(gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc0ec7-38ec-42b3-bfa0-0c04be6ba39b",
   "metadata": {},
   "source": [
    "+ 학습률이 너무 크면 큰 값으로 발산 / 너무 작으면 거의 갱신되지 않은 채 종료"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed115b-5a60-4e29-bc33-cc733f476537",
   "metadata": {},
   "source": [
    "### 4-4-2. 신경망에서의 기울기\n",
    "\n",
    "신경망 학습 역시 가중치 매개변수에 대한 손실 함수의 기울기를 구해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "614c4483-80d1-4908-b25b-3e2fe6819c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.getcwd(), 'dataset'))\n",
    "from functions import softmax, cross_entropy_error\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "class simpleNet: # 형상이 2x3인 가중치 매개변수 하나를 변수로 갖음\n",
    "    def __init__(self):\n",
    "        self.W=np.random.randn(2,3) # 정규분포로 초기화 \n",
    "\n",
    "    def predict(self, x): # 예측을 수행하는 매서드\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t): # 손실 함수의 값을 구하는 매서드 x: 입력 데이터 t: 정답 레이블\n",
    "        z=self.predict(x)\n",
    "        y=softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5850455f-4556-4374-8782-28cb0d329dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07903218 0.23366711 1.01431417]\n",
      " [0.83294105 0.6804663  0.47111675]]\n",
      "[0.79706626 0.75261993 1.03259358]\n",
      "2\n",
      "0.9345066448492001\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W) # 가중치 매개변수 \n",
    "x=np.array([0.6,0.9])\n",
    "p=net.predict(x)\n",
    "print(p)\n",
    "print(np.argmax(p)) # 최댓값의 인덱스 \n",
    "\n",
    "t=np.array([0,0,1]) # 정답 레이블 \n",
    "print(net.loss(x,t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2256cab0-b54b-4be3-9202-7397cf58e363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18621374  0.17811846 -0.36433221]\n",
      " [ 0.27932062  0.2671777  -0.54649831]]\n"
     ]
    }
   ],
   "source": [
    "def f(w):\n",
    "    return net.loss(x,t)\n",
    "\n",
    "dw=numerical_gradient(f,net.W)\n",
    "print(dw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c97c8-9f77-44d9-9e8e-e851148b5fc9",
   "metadata": {},
   "source": [
    "### 4-5. 학습 알고리즘 구현하기\n",
    "\n",
    "신경망 학습: 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정 아래의 4과정을 거침\n",
    "\n",
    "1. 미니배치 추출: 훈련 데이터 중 일부를 무작위로 가져온 것으로 미니배치의 손실 함수 값을 줄이는 것이 목표\n",
    "2. 기울기 산출: 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수 의 값을 가장 적게 하는 방향을 제시\n",
    "3. 매개변수 갱신: 가중치 매개변수를 기울기 방향으로 아주 조금 갱신\n",
    "4. 반복: 1~3단계 반복\n",
    "\n",
    "-> 데이터를 미니배치로 무작위로 선정하기 때문에 확률적 경사 하강법(SGD)이라 부름."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d4ee1693-3456-4f2c-8d7c-cbd855c4476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import * \n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                 weight_init_std=0.01):\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params={}\n",
    "        self.params['W1']=weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1']=np.zeros(hidden_size)\n",
    "        self.params['W2']=weight_init_std * np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2']=np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2=self.params['W1'], self.params['W2']\n",
    "        b1, b2=self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1=np.dot(x,W1) + b1\n",
    "        z1=sigmoid(a1)\n",
    "        a2=np.dot(z1,W2) + b2\n",
    "        y=softmax(a2)\n",
    "\n",
    "        return y\n",
    "    # x: 입력 데이터, t: 정답 레이블 \n",
    "    def loss(self, x, t):\n",
    "        y=self.predict(x)\n",
    "\n",
    "        return cross_entropy_error(y,t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y=self.predict(x)\n",
    "        y=np.argmax(y, axis=1)\n",
    "        t= np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy=np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    # x: 입력 데이터, t:정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W:self.loss(x,t)\n",
    "\n",
    "        grads={}\n",
    "        grads['W1']=numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1']=numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2']=numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2']=numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ccbb1d-dd58-4246-8d1f-99dda521c2d0",
   "metadata": {},
   "source": [
    "+ params: 신경망의 매개변수를 보관하는 딕셔너리 변수, params[W1]은 1번째 층의 가중치, params['b1']은 1번 층의 편향\n",
    "+ grads: 기울기 보관하는 딕셔너리 변수 매서드의 반환 값, grads['W1']은 1번째 층의 가중치 기울기, grads['b1']은 1번째 층의 편향의 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5827ad00-66ea-4b01-a2c0-bf32436409e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net=TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "247b9bda-042d-4957-822b-84adacc5c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.random.rand(100,784) # 더미 입력 데이터(100장 분량)\n",
    "y=net.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "22428fff-605e-42cb-87fc-93bf66a2d042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "x=np.random.rand(100,784) # 더미 입력 데이터(100장 분량)\n",
    "t=np.random.rand(100,10) # 더미 정답 레이블(100장 분량)\n",
    "\n",
    "grads=net.numerical_gradient(x,t) # 기울기 계산 \n",
    "\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a0b0d3-7d49-4908-ae80-64feba8e3ec5",
   "metadata": {},
   "source": [
    "### 4-5-2. 미니배치 학습 구현하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c6030542-3d8e-40b1-8b29-8e40ec6c818e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n(x_train, t_train),(x_test,t_test)=load_mnist(normalize=True, one_hot_label=True)\\n\\ntrain_loss_list=[]\\n\\n# 하이퍼 파라미터\\niters_num=10000 # 반복 횟수 \\ntrain_size=x_train.shape[0]\\nbatch_size=100 # 미니배치 크기\\nlearning_rate=0.1 # 학습률 \\nnetwork=TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\\n\\nfor i in range(iters_num):\\n    # 미니배치 획득 \\n    batch_mask=np.random.choice(train_size, batch_size)\\n    x_batch=x_train[batch_mask]\\n    t_batch=t_train[batch_mask]\\n\\n    # 기울기 계산\\n    grad=network.numerical_gradient(x_batch, t_batch)\\n\\n    # 매개변수 갱신\\n    for key in ('W1','b1','W2','b2'):\\n        network.params[key] -=learning_rate * grad[key]\\n\\n    # 학습 경과 기록 \\n    loss=network.loss(x_batch, t_batch)\\n    train_loss_list.append(loss)\\n\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "(x_train, t_train),(x_test,t_test)=load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list=[]\n",
    "\n",
    "# 하이퍼 파라미터\n",
    "iters_num=10000 # 반복 횟수 \n",
    "train_size=x_train.shape[0]\n",
    "batch_size=100 # 미니배치 크기\n",
    "learning_rate=0.1 # 학습률 \n",
    "network=TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득 \n",
    "    batch_mask=np.random.choice(train_size, batch_size)\n",
    "    x_batch=x_train[batch_mask]\n",
    "    t_batch=t_train[batch_mask]\n",
    "\n",
    "    # 기울기 계산\n",
    "    grad=network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key] -=learning_rate * grad[key]\n",
    "\n",
    "    # 학습 경과 기록 \n",
    "    loss=network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba4163a-303c-4a96-9753-43eb8f54dced",
   "metadata": {},
   "source": [
    "미니배치 크기를 100으로 하여, 매번 6만개의 훈련 데이터에서 임의로 100개를 추린다. 그 후 100개의 미니배치를 대상으로 SGD를 수행해 매개변수를 갱신한다.\n",
    "\n",
    "갱신 횟수를 만번으로 하여, 갱신할 때마다 훈련 데이터에 대한 손실 함수를 계산하여 그 값을 배열에 추가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761bc0a7-53e8-459a-b09a-9bf2b0bc0760",
   "metadata": {},
   "source": [
    "### 4-5-3. 시험 데이터로 평가하기\n",
    "\n",
    "위의 코드에서 구하는 손실 함수 의 값은 훈련 데이터의 미니배치에 대한 손실 함수 값으로, 다른 데이터셋에서도 비슷한 성능이 나오는지 확인해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a0d1db4c-fdb1-4c96-8e9e-b7a204a90d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnetwork=TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\\n\\n# 하이퍼파라미터\\niters_num=10000 # 반복 횟수 \\ntrain_size=x_train.shape[0]\\nbatch_size=100 # 미니배치 크기\\nlearning_rate=0.1 # 학습률 \\n\\ntrain_loss_list=[]\\ntrain_acc_list=[]\\ntest_acc_list=[]\\n\\n# 1에포크당 반복 수 \\niter_per_epoch=max(train_size/batch_size,1)\\n\\nfor i in range(iters_num):\\n    # 미니배치 획득 \\n    batch_mask=np.random.choice(train_size, batch_size)\\n    x_batch=x_train[batch_mask]\\n    t_batch=t_train[batch_mask]\\n\\n    # 기울기 계산\\n    grad=network.numerical_gradient(x_batch, t_batch)\\n\\n    # 매개변수 갱신\\n    for key in (\\'W1\\',\\'b1\\',\\'W2\\',\\'b2\\'):\\n        network.params[key] -=learning_rate * grad[key]\\n\\n    # 학습 경과 기록 \\n    loss=network.loss(x_batch, t_batch)\\n    train_loss_list.append(loss)\\n\\n    # 1 에포크당 정확도 계산 \\n    if i % iter_per_epoch==0:\\n        train_acc=network.accuracy(x_train, t_train)\\n        test_acc=network.accuracy(x_test, t_test)\\n        train_acc_list.append(train_acc)\\n        test_acc_list.append(test_acc)\\n        print(\"train acc\" + str(train_acc) +\", test acc \" + str(test_acc))\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "network=TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num=10000 # 반복 횟수 \n",
    "train_size=x_train.shape[0]\n",
    "batch_size=100 # 미니배치 크기\n",
    "learning_rate=0.1 # 학습률 \n",
    "\n",
    "train_loss_list=[]\n",
    "train_acc_list=[]\n",
    "test_acc_list=[]\n",
    "\n",
    "# 1에포크당 반복 수 \n",
    "iter_per_epoch=max(train_size/batch_size,1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득 \n",
    "    batch_mask=np.random.choice(train_size, batch_size)\n",
    "    x_batch=x_train[batch_mask]\n",
    "    t_batch=t_train[batch_mask]\n",
    "\n",
    "    # 기울기 계산\n",
    "    grad=network.numerical_gradient(x_batch, t_batch)\n",
    "\n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1','b1','W2','b2'):\n",
    "        network.params[key] -=learning_rate * grad[key]\n",
    "\n",
    "    # 학습 경과 기록 \n",
    "    loss=network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "\n",
    "    # 1 에포크당 정확도 계산 \n",
    "    if i % iter_per_epoch==0:\n",
    "        train_acc=network.accuracy(x_train, t_train)\n",
    "        test_acc=network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc\" + str(train_acc) +\", test acc \" + str(test_acc))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc3d65f-0a15-4615-9ea9-8b0b7b3207a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
